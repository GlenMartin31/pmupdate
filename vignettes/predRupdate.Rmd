---
title: "Introduction to the predRupdate package"
author: Glen P. Martin, PhD; David Jenkins, PhD; Matthew Sperrin, PhD
output: 
  rmarkdown::html_vignette:
    toc: yes
vignette: >
  %\VignetteIndexEntry{Introduction to the predRupdate package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, echo = FALSE}
library(predRupdate)
```

# Preamble
The __predRupdate__ package includes a set of functions to aid in the validation of a clinical prediction model (CPM) on a given dataset, and to apply various model updating and aggregation methods. This vignette aims to overview, through examples, some common workflows of using the __predRupdate__ package. For a technical vignette describing the methods underpinning the package, please see `vignette("predRupdate_technical")`. 

The package is focused on the situation where there is an existing CPM (or multiple CPMs) that has been developed (e.g., a model published in the literature), and one wishes to apply this model to a new dataset. We foresee at least three use-cases: (1) where one wishes to externally validate the existing CPM on the new data to tests the model's predictive performance, i.e., external validation; (2) where one wishes to apply model updating methods to the existing CPM to 'tailor' it to the new dataset; and (3) where there are multiple existing CPMs, and one wishes to apply model aggregation (meta-modelling) methods to pool these models into a single model for the new dataset. We therefore give three examples below for each of these use cases. 

# Data
The data used throughout this vignette are available within the package, called _SYNPM_. These data are available to users of __predRupdate__ after installing and loading the package. See "?SYNPM" for details of these data. In short, the data and models included in _SYNPM_ are synthetic, but for the purposes of this vignette, we imagine that one is interested in predicting someone's risk of mortality after surgery. Data are available on `r nrow(SYNPM$ValidationData)` people, which records each individuals age, gender, smoking status, diabetes status, and chronic kidney disease (CKD) status at the time of surgery. The data includes the outcomes of "ETime" representing the time (months) between surgery and either death or end-of-follow-up (5 months), whichever occurred first. The variable "Status" indicates if the patient died (1) or was right-censored (0), and Y denotes a binary variable indicating if the patient died within 1 month. 

# Example 1: validating an existing prediction model on new data
In this first example, we take a situation where a CPM has previously been developed (in another dataset) to predict the risk of mortality within 1 month of surgery, and we wish to validate this model in our dataset to test the predictive performance (e.g., an external validation study). 

The existing model was a logistic regression model, with the following predictor variables and coefficients (log-odds ratios):

```{r, echo = FALSE}
coefs_table <- as.data.frame(round(t(SYNPM$Existing_logistic_models[1,which(!is.na(SYNPM$Existing_logistic_models[1,]))]), 3))
names(coefs_table) <- c("Coefficient")
knitr::kable(coefs_table, caption = "Table of coefficients for the existing logistic regression prediction model")
```

The first step in using __predRupdate__ to validate this model is to input the model information. We start by creating a data.frame of the model coefficients, with the columns being the predictor variable names. This information is then passed into the "pred_input_info()" function to input the information about the existing model. See the help file of "pred_input_info()" for details.
```{r}
# create a data.frame of the model coefficients, with columns being variables
coefs_table <- data.frame("Intercept" = -3.4, #the intercept needs to be named exactly as given here
                          "SexM" = 0.306, 
                          "Smoking_Status" = 0.628,
                          "Diabetes" = 0.499,
                          "CKD" = 0.538)

#pass this into pred_input_info()
Existing_Logistic_Model <- pred_input_info(model_type = "logistic",
                                           model_info = coefs_table)
summary(Existing_Logistic_Model)
```

Next we wish to apply this model to our dataset to calculate the predicted risks for each individual, and then compare these predictions with the observed outcomes to calculate predictive performance. This can all be achieved with the "pred_validate()" function, as follows:
```{r}
pred_validate(x = Existing_Logistic_Model,
              new_data = SYNPM$ValidationData,
              binary_outcome = "Y")
```

This produces a flexible calibration plot, along with outputting various metrics of model calibration (e.g., calibration intercept and slope),  discrimination (e.g., area under the ROC curve) and overall performance (e.g., R-squared). We can see that this model has poor calibration (calibration plot deviating from the y=x line, with calibration intercept and slope significantly different from 0 and 1, respectively), and poor discrimination. One may wish to update this model to the new dataset - see Example 2 below.

## Survival analysis model
The above example considered the validation of an existing CPM that was based on logistic regression. __predRupdate__ also contains functionality to validate CPMs that are based on time-to-event (survival) models (e.g. a Cox proportional hazards model). In such a case, the baseline cumulative hazard of the model should also be specified, along with the regression coefficients. 

In many cases, the baseline cumulative hazard of an existing CPM will not be reported in "full", but rather estimates of the baseline cumulative hazard will be given at set follow-up times. To use __predRupdate__, users should specify the baseline cumulative hazard at the times at which one wishes to make predictions (or validate/update the model). 

For example, suppose an existing CPM was developed using Cox proportional hazards to predict time-to-death after surgery, with the following predictor parameters (log hazard ratios):
```{r, echo = FALSE}
coefs_table <- as.data.frame(round(t(SYNPM$Existing_TTE_models[1,which(!is.na(SYNPM$Existing_TTE_models[1,]))]), 3))
names(coefs_table) <- c("Coefficient")
knitr::kable(coefs_table, caption = "Table of coefficients for the existing time-to-event regression prediction model")
```

and with the following baseline cumulative hazard reported at discrete times of months 1-5 post surgery:
```{r, echo = FALSE}
BH_table <- SYNPM$TTE_mod1_baseline
knitr::kable(BH_table, caption = "Table of baseline cumulative hazard")
```

We can then use the "pred_validate()" function to validate this model at 1, 2, 3, 4 or 5 months follow-up in the new dataset. To achieve this, one follows a similar syntax to above for the logistic model. The main difference is that one needs to specify a time during follow-up that we'd like to validate the model against - this time must also be available in the baseline cumulative hazard provided. 
```{r}
# create a data.frame of the model coefficients, with columns being variables
coefs_table <- data.frame("Age" = 0.008,
                          "SexM" = 0.257,
                          "Smoking_Status" = 0.697,
                          "Diabetes" = 0.310,
                          "CKD" = 0.636)

#pass this into pred_input_info()
Existing_TTE_Model <- pred_input_info(model_type = "survival",
                                      model_info = coefs_table,
                                      cum_hazard = BH_table) #where BH_table is the baseline hazard above

#now validate against the time-to-event outcomes in the new dataset:
pred_validate(x = Existing_TTE_Model,
              new_data = SYNPM$ValidationData,
              survival_time = "ETime",
              event_indicator = "Status",
              time_horizon = 5)
```

### Specifying the baseline cumulative hazard
In the above example, we assumed that the baseline cumulative hazard of the existing CPM was available at discrete follow-up times. In some situations, the entire baseline cumulative hazard curve may be presented, or indeed a parametric form of the baseline cumulative hazard may be provided. In such a situation, one should extract (from the plot) or calculate (from the parametric form) the baseline cumulative hazard at multiple follow-up times of interest (the times at which one wishes to validate the model at).  

Additionally, in some cases, the baseline cumulative hazard of an existing survival (time-to-event) CPM may not be reported/available. In such a situation, one can still use __predRupdate__ to validate such a model. However, only a limited number of metrics will be produced (i.e., only those that require the linear predictor, not absolute risk predictions at a given follow-up time). Specifically, the observed:expected ration and the calibration plot will not be produced. 

For example, suppose the baseline cumulative hazard for the above model was not avaliable. We could validate this model using __predRupdate__ as follows:
```{r}
# create a data.frame of the model coefficients, with columns being variables
coefs_table <- data.frame("Age" = 0.008,
                          "SexM" = 0.257,
                          "Smoking_Status" = 0.697,
                          "Diabetes" = 0.310,
                          "CKD" = 0.636)

#pass this into pred_input_info()
Existing_TTE_Model <- pred_input_info(model_type = "survival",
                                      model_info = coefs_table,
                                      cum_hazard = NULL) #leave as NULL if the baseline not avaliable

#now validate against the time-to-event outcomes in the new dataset:
pred_validate(x = Existing_TTE_Model,
              new_data = SYNPM$ValidationData,
              survival_time = "ETime",
              event_indicator = "Status",
              time_horizon = 5)
```

Here, we see that no calibration plot is produced, and the observed:expected ratio is NA. A warning message is given to highlight this.

# Example 2: model updating on new data
In the validation of an existing logistic regression model in Example 1 above, we found that the existing model was miscalibrated in the new data. One strategy to handle this is to apply model updating methods. See `vignette("predRupdate_technical")` for a technical discussion of these methods. 
We here choose to apply model re-calibration; to do so, we can use the "pred_update()" function:
```{r}
# create a data.frame of the model coefficients, with columns being variables
coefs_table <- data.frame("Intercept" = -3.4,
                          "SexM" = 0.306,
                          "Smoking_Status" = 0.628,
                          "Diabetes" = 0.499,
                          "CKD" = 0.538)

#pass this into pred_input_info()
Existing_Logistic_Model <- pred_input_info(model_type = "logistic",
                                           model_info = coefs_table)

#apply the pred_update function to update the model to the new dataset:
Updated_model <- pred_update(Existing_Logistic_Model,
                             update_type = "recalibration",
                             new_data = SYNPM$ValidationData,
                             binary_outcome = "Y")
```

One could then validate this updated model using "pred_validate()", but given we have updated the model in the new data, then any such performance estimates would need to be adjusted for in-sample optimism (e.g., using cross-validation or bootstrap internal validation):
```{r}
pred_validate(Updated_model, 
              new_data = SYNPM$ValidationData, 
              binary_outcome = "Y")
```

Similar functionality is available for time-to-event models.

# Example 3: model aggregation on new data
Sometimes, there are instances where multiple existing CPMs are available for the same prediction task (e.g., existing models developed across different countries). Here, model aggregation methods can be used to pool these existing CPMs into a single model in the new data. See `vignette("predRupdate_technical")` for a technical discussion of these methods.

To implement this, we input the information about multiple models into "pred_input_info()"; each row of the model_info parameter should be the coefficients of the existing models. Any parameter that is not included in a given CPM should have value NA, as shown below:
```{r}
coefs_table <- data.frame(rbind(c("Intercept" = -3.4,
                                  "SexM" = 0.306,
                                  "Smoking_Status" = 0.628,
                                  "Diabetes" = 0.499,
                                  "CKD" = 0.538,
                                  "Age" = NA),
                                c("Intercept" = -4.748,
                                  "SexM" = 0.196,
                                  "Smoking_Status" = 0.492,
                                  "Diabetes" = 0.350,
                                  "CKD" = 0.555,
                                  "Age" = 0.049),
                                c("Intercept" = -3.267,
                                  "SexM" = NA,
                                  "Smoking_Status" = 0.477,
                                  "Diabetes" = NA,
                                  "CKD" = 0.444,
                                  "Age" = NA)))
multiple_mods <- pred_input_info(model_type = "logistic",
                                 model_info = coefs_table)
summary(multiple_mods)
```

We can then use "pred_stacked_regression()" function to apply stacked regression to pool these models into a single model for the new dataset:
```{r}
SR <- pred_stacked_regression(x = multiple_mods,
                              new_data = SYNPM$ValidationData,
                              binary_outcome = "Y")
summary(SR)
```

One could then validate this updated model using "pred_validate()", but given we have updated the model in the new data, then any such performance estimates would need to be adjusted for in-sample optimism (e.g., using cross-validation or bootstrap internal validation):
```{r}
pred_validate(SR, 
              new_data = SYNPM$ValidationData, 
              binary_outcome = "Y")
```

Similar functionality is available for time-to-event models.
