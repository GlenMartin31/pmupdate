% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/validate_probabilities.R
\name{validate_probabilities}
\alias{validate_probabilities}
\title{Validate predicted probabilities against binary outcome}
\usage{
validate_probabilities(
  ObservedOutcome,
  Prob,
  LP,
  CalPlot = c("smooth", "grouped", "none"),
  groups = NULL,
  xlab = "Predicted Probability",
  ylab = "Observed Probability",
  xlim = c(0, 1),
  ylim = c(0, 1)
)
}
\arguments{
\item{ObservedOutcome}{a vector of N binary observations, denoting if the
outcome was observed (1) or not observed (0) for each individual in the
validation dataset}

\item{Prob}{a vector of N predicted probabilities from the model. Specify
either \code{Prob} or \code{LP}.}

\item{LP}{a vector of N observations where each is the calculated linear
predictor (log-odds) from the existing model that is being evaluated.
Specify either \code{Prob} or \code{LP}.}

\item{CalPlot}{Indicates whether a calibration plot should be produced, and
the method for doing so. Set to smooth (default) if a flexible (smooth)
calibration plot should be produced using natural cubic splines, set to
grouped if a grouped/binned calibration plot should be produced, and set to
none if no calibration plot should be produced. If set to grouped then
\code{groups} specifies the number of groups to produce.}

\item{groups}{Specifies the number of groups to use for a grouped/binned
calibration plot. Leave as default (NULL) if either no calibration plot or
a flexible calibration plot is selected.}

\item{xlab, ylab, xlim, ylim}{Specifies plotting characteristics for the
calibration plot, if relevant.}
}
\value{
Returns a list of the performance metrics and associated 95\%
confidence intervals, where appropriate.
}
\description{
This function is used to validate predicted probabilities (usually from an
existing/ previously developed logistic regression model) against binary
outcomes. It takes a vector of predicted risks (or the linear predictor) and
a vector of binary observed outcomes, from which the function calculates
metrics of calibration, discrimination and overall accuracy.
}
\details{
This function assesses the predictive performance of predicted risks
against an observed binary outcome. Various metrics of calibration
(agreement between the observed risk and the predicted risks, across the
full risk range) and discrimination (ability of the model to distinguish
between those who develop the outcome and those who do not). For
calibration, a calibration plot is produced, using either flexible methods
or the binned/grouped approach. Calibration-in-the-large (CITL) and
calibration slopes are also estimated. For CITL, we estimate the intercept
by fitting a logistic regression model to the observed binary outcomes,
with the linear predictor of the model as an offset. For calibration slope,
a logistic regression model is fit to the observed binary outcome with the
linear predictor from the model as the only covariate. For discrimination,
we estimate the area under the receiver operating characteristic curve
(AUC). Various other metrics are also calculated to assess overall accuracy
(Brier score, Cox-Snell R2).
}
\seealso{
\code{\link{pred_validate}}, \code{\link{pred_predict}},
\code{\link{pred_input_info}}
}
