% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pred_validate.R
\name{pred_validate}
\alias{pred_validate}
\title{Validate an existing prediction}
\usage{
pred_validate(
  x,
  newdata,
  binary_outcome = NULL,
  survival_time = NULL,
  event_indicator = NULL,
  time_horizon = NULL,
  ...
)
}
\arguments{
\item{x}{an object of class "predinfo"}

\item{newdata}{data.frame upon which the prediction model should be validated}

\item{binary_outcome}{Character variable giving the name of the column in
\code{newdata} that represents the observed outcomes. Only relevant for
\code{model_type}="logistic"; leave as \code{NULL} otherwise.}

\item{survival_time}{Character variable giving the name of the column in
\code{newdata} that represents the observed survival times. Only relevant
for \code{model_type}="survival"; leave as \code{NULL} otherwise.}

\item{event_indicator}{Character variable giving the name of the column in
\code{newdata} that represents the observed survival indicator (1 for
event, 0 for censoring). Only relevant for \code{model_type}="survival";
leave as \code{NULL} otherwise.}

\item{time_horizon}{for survival models, an integer giving the time horizon
(post baseline/time of prediction) at which a prediction is required.
Currently, this must match a time in x$baselinehazard.}

\item{...}{further arguments passed to other methods. See Details below.}
}
\value{
A list of performance metrics, estimated by applying the existing
prediction model to the newdata.
}
\description{
Validate an existing prediction model, to calculate the predictive
performance against a new (validation) dataset.
}
\details{
This function takes an existing prediction model formatted according
to \code{\link{pred_input_info}}, and calculates measures of predictive
performance on new data (e.g., within an external validation study). The
information about the existing prediction model should first be inputted by
calling \code{\link{pred_input_info}}, before passing the resulting object
to \code{pred_validate}.

\code{newdata} should be a data.frame, where each row should be an
observation (e.g. patient) and each variable/column should be a predictor
variable. The predictor variables need to include (as a minimum) all of the
predictor variables that are included in the existing prediction model
(i.e., each of the variable names supplied to
\code{\link{pred_input_info}}, through the \code{model_info} parameter,
must match the name of a variables in \code{newdata}).

Any factor variables within \code{newdata} must be converted to dummy (0/1)
variables before calling this function. \code{\link{dummyvars}} can help
with this - see examples below.

\code{binary_outcome}, \code{survival_time} and \code{event_indicator} are
used to specify the outcome variable(s) within \code{newdata} (use
\code{binary_outcome} if \code{x$model_type} = "logistic", or use
\code{survival_time} and \code{event_indicator} if \code{x$model_type} =
"survival").

In the case of validating a logistic regression model, this function
assesses the predictive performance of the predicted risks against an
observed binary outcome. Various metrics of calibration (agreement between
the observed risk and the predicted risks, across the full risk range) and
discrimination (ability of the model to distinguish between those who
develop the outcome and those who do not) are calculated. For calibration,
a calibration plot is produced, using either flexible methods or the
binned/grouped approach. Calibration-in-the-large (CITL) and calibration
slopes are also estimated. For CITL, we estimate the intercept by fitting a
logistic regression model to the observed binary outcomes, with the linear
predictor of the model as an offset. For calibration slope, a logistic
regression model is fit to the observed binary outcome with the linear
predictor from the model as the only covariate. For discrimination, we
estimate the area under the receiver operating characteristic curve (AUC).
Various other metrics are also calculated to assess overall accuracy (Brier
score, Cox-Snell R2). Specify parameter \code{CalPlot} to indicate whether
a calibration plot should be produced, and the method for doing so; set to
"smooth" (default) if a flexible (smooth) calibration plot should be
produced using natural cubic splines, set to "grouped" if a grouped/binned
calibration plot should be produced, and set to "none" if no calibration
plot should be produced. If set to grouped then specification of parameter
\code{groups} specifies the number of groups to produce. Can also specify
parameters \code{xlab}, \code{ylab}, \code{xlim},and \code{ylim} to change
plotting characteristics for the calibration plot.

In the case of validating a survival prediction model,...
}
\examples{
#Example 1 - logistic regression existing model, with outcome specified; uses
#            an example dataset within the package
model1 <- pred_input_info(model_type = "logistic",
                          model_info = SYNPM$Existing_models[1,])
pred_validate(x = model1,
              newdata = SYNPM$ValidationData,
              binary_outcome = "Y")

#Example 2 - survival model example; uses an example dataset within the
#             package. Also shows use of pre-processing to handle
#             categorical variables - need converting prior to call
SMART_dummaryvars <- dummyvars(SMART$SMART_dataset)
model2 <- pred_input_info(model_type = "survival",
                          model_info = SMART$Existing_models[1,],
                          baselinehazard = SMART$Framingham_Male_baseline)
pred_validate(x = model2,
             newdata = SMART_dummaryvars,
            survival_time = "TEVENT",
            event_indicator = "EVENT",
            time_horizon = 10)

#Example 3 - multiple existing models
model3 <- pred_input_info(model_type = "logistic",
                          model_info = SYNPM$Existing_models)
pred_validate(x = model3,
              newdata = SYNPM$ValidationData,
              binary_outcome = "Y")

}
\seealso{
\code{\link{pred_input_info}}
}
